\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{algorithm2e}
\usepackage{rotating}
\usepackage[space]{grffile}
\usepackage[font=small,skip=0pt]{caption}
%\DeclareMathOperator{\Tr}{Tr}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\graphicspath{ {figures/} }
% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% custom commands
\newcommand{\scream}[1]{{\color{red} \bf *** #1 ***}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint}
%\title{Enrich Object Detection : 2D-3D registration and continuous viewpoint estimation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
A large body of recent work on object detection has focused on exploiting 3D CAD
model databases to improve detection performance. Many of these approaches work
by aligning exact 3D models to images using templates generated from renderings
of the 3D models at a finite set of discrete viewpoints. The training procedures
for these approaches, however, are very expensive and require gigabytes of
memory and storage, and the viewpoint discretization hampers pose estimation
performance.

We propose an efficient method for synthesizing templates from 3D models that
runs on the fly -- that is, it quickly produces detectors for an arbitrary
viewpoint of a 3D model without expensive dataset-dependent training or template
storage. Given a 3D model and an arbitrary continuous detection viewpoint, our
method synthesizes a discriminative template by extracting features from a
rendered view of the object and decorrelating spatial dependences among the
features. Our decorrelation procedure relies on a gradient-based algorithm that
is more numerically stable than standard decomposition-based procedures, and we
efficiently search for candidate detections by computing FFT-based template
convolutions. Due to the speed of our template synthesis procedure, we are able
to perform joint optimization on continuous scale, translation, rotation, and
focal length. We provide an efficient GPU implementation of our algorithm, and
we validate its performance on 3DObject dataset and PASCAL 2012 dataset.

%     Using 3D model to detect and register the model to RGB image has been a
%     growing field of study. Recently, Aubry \etal \cite{Aubry14} and J. Lim
%     \etal \cite{Lim14} tried to estimate pose and align exact CAD model to an
%     image using part-based templates. Malisiewicz \etal \cite{Malisiewicz11}
%     also addresses the issue by transferring metadata after detection. Hejrati
%     \etal \cite{Hejrati14} uses template synthesis to detect an object. All
%     these approaches require extensive training, a lot of memory space. Also,
%     the training based approach requires defining fixed viewpoints which limit
%     themselves.  To overcome these difficulties, we propose an efficient and
%     fast way to synthesize and validate templates using on the fly realistic
%     rendering that can jointly optimize scale, translation, rotation and focal
%     length continuously. We render an object and extract features and
%     decorrelate spatial dependencies within them on the fly to make a
%     discriminative template. To speed up the decorrelation procedure, we adopt
%     a gradient based algorithm which is numerically more stable than
%     decomposition procedure. Finally, we efficiently search for candidate
%     match by computing convolution using FFT and jointly optimize to match the
%     object accurately. To speed up further, we propose an efficient GPU
%     implementation and tested on PASCAL images.

%   In contrast to other mid-level patch based 3D matching methods which
%   requires extensive training and more than 10Gb of memory space and physical
%   storage, ours require no training nor saving the templates thus enabling it
%   to run on any personal computer.  To overcome these difficulties, we propose
%   an efficient and fast way to synthesize and validate templates using on the
%   fly realistic rendering that can jointly optimize scale, translation,
%   rotation and focal length continuously. We render an object and extract
%   features and decorrelate spatial dependencies within them on the fly to make
%   a discriminative template. To speed up the decorrelation procedure, we adopt
%   a gradient based algorithm which is numerically more stable than
%   decomposition procedure. Finally, we efficiently search for candidate match
%   by computing convolution using FFT and jointly optimize to match the object
%   accurately. To speed up further, we propose an efficient GPU implementation
%   and tested on PASCAL images.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Related Works}

% Using HOG
% In this paper, we also used Histograms of Orientations feature \cite{Dalal05} but in general, we could use any dense features. 

\subsection{Object Detection and Viewpoint Estimation}

Modern object detectors generalize very well, handling intraclass variability,
occlusion, truncation and viewpoint changes \cite{Felzenszwalb10, Girshick14}.
However, this generalization comes at the cost of fine-grained information,
including accurate 3D pose and object sub-category recognition. Such methods
typically produce bounding box detection hypotheses, with little further
information.

Many methods have attempted to move object detection towards richer outputs,
especially by jointly performing detection and pose estimation \cite{Pepik12,
Xiang12, Fidler12, Xiang14, Hejrati14, Aubry14, Lim14}. To achieve this,
\cite{Xiang12, Hejrati14, Fidler12} use 3D representations that deform as
viewpoint changes and \cite{Pepik12} uses geometric constraints to regularize 2D
appearance models.

The methods above perform discrete pose estimation, quantizing the viewing
sphere into a number of poses and selecting the best one during inference.
Fine-grained pose estimators, in contrast, can infer continuous (or arbitrarily
fine-grained) poses. One such method from \cite{Zia13} aligns a 3D deformable
part-based wireframe model with input images to accurately predict object poses.

More recently, \cite{Aubry14, Lim14} made progress in joint instance-level
object detection and pose estimation. To estimate pose they use synthetic
renderings of CAD models to learn discriminative mid-level patches.
\cite{Aubry14} calibrates these patches on a small set of real images, while
\cite{Lim14} present a method for learning the relative discriminativeness of
the patches.

\textcolor{red}{Again, should we highlight the differences between these methods and ours?}

% specialized in giving such accurate information, can be applied on any generic
% object detectors to give high quality metadata.  Many Since the WHO does not
% require extensive training such as \cite{Felzenszwalb10, Malisiewicz11,
% Girshick14}, WHO is gaining momentum in 2D-3D matching. \cite{Aubry13,
% Aubry14, Lim14} combined WHO with synthetic rendering to jointly detect and
% estimate pose by making mid-level patches using WHO. Using this WSuch approach
% has an advantage over generic detections. First, it is easy to trace the
% detection to specific template. The template also has The rendering has
% corresponding CAD model and viewpoint thus enabling joint detection and
% viewpoint estimation.  To deal with textureless background \cite{Aubry13}
% whitens HOG feature and then zero out the WHO feature where there is no
% texture. But centering a HOG feature where there is no edge creates strong
% negative edges. We found out that centering only non-zero cells help detection
% but whitening all cell makes features to leak out to texture-less regions.
% With increasing popularity of rendering image, whitening, traditional signal
% processing technique, as a preprocessing stage has been widely used also in
% computer vision community.  Many object detectors \cite{Felzenszwalb10,
% Malisiewicz11} \subsection{Mid-level Patches for 2D-3D matching} FPM
% introduction : per instance matching  Seeing 3D chair : mid-level patches
  
%\begin{comment}
%  \subsection{Contributions}
 % \begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
  %  \item Introduces Non-Zero Whitened Histograms of Orientations (NZ-WHO) for real-time template generation
   % \item Continuous viewpoint and focal length tuning using on-the-fly template generation
%    \item Jointly optimizes translation, 3D viewpoint, CAD models and focal length
%     \begin{comment}
%       \item HOG Whitening procedure that do not require calibration stage
%     \end{comment}
%  \end{enumerate}
% \end{comment}
% All the code and CAD models is publicly available online \cite{Choy14} (currently not disclosed for anonymous review, visualizations are available in the supplementary material).

\section{Our Approach \scream{better name?}}
\label{sec:nz-who}

\subsection{Overview}
Our method attempts to match CAD models to 2D images, determining the best model within a class, its pose and scale, and the focal length of the camera used to produce the image. These parameters are first coarsely estimated using a discrete bank of detectors trained offline, before they are refined using MCMC by generating new detectors on the fly.

The basic detection method underlying our approach is HOG template matching. However, we propose a new HOG variant called Non-Zero Whitened Histograms of Orientations (NZ-WHO), where the HOG templates are whitened using statistics found from a random collection of natural images, producing a more discriminative template.

In addition to the library of CAD models that will be matched to the query images, our method requires a collection of random natural images. Offline, we compute HOG features of these images and record their mean and spatial autocovariance. The images are not used during testing, only the statistics generated from them.

Notably, we do not require any labelled training images of the objects to be detected. Our templates are generated exclusively by rendering CAD models from our library. From these renderings we produce novel HOG-like features we call Non-Zero Whitened Histograms of Orientations (NZ-WHO), which use the natural image statistics found offline to whiten a standard HOG template, making it more discriminative.

The matching proceeds in two steps: First, we apply a bank of pre-trained detectors to the image, corresponding to a set of discrete viewpoints, scales, and object models. This provides a rough estimate of these parameters. Second, we refine these parameters using MCMC, where each candidate is evaluated by rendering a new whitened template on the fly. 

Optionally, our algorithm can be initialized using proposal bounding boxes from state-of-the-art object detectors \cite{Felzenszwalb10,Girshick14} before executing the two steps above. In this scenario, our approach provides a means to augment standard bounding box detectors with more interesting 3D information.

\begin{comment}
Our method is in line with \cite{Aubry14, Lim14} in the sense that we
use only renderings, not real images. However, rather than using fixed
sized mid-level patches, we use whole objects and make root
templates. By using root templates, there is no need to learn a
discriminative patch dictionary or to calibrate the patches and an
additional inference stage is not required. However, whitening the
whole template is prohibitive for several reasons. All templates have
different aspect ratios making it difficult to generate the covariance
matrix $\Sigma$. Since the root templates are larger than mid-level
patches, a very large auto-covariance $\Gamma$ is required. Lastly,
time complexity of decorrelating a template with $n$ cells increases
as $O(n^3)$, resulting in slow template generation and large residuals
due to numerical errors accumulated while decomposing the covariance
matrix $\Sigma$. 

To overcome these difficulties, we propose the Non-Zero Whitened
Histograms of Orientations (NZ-WHO) with iterative Conjugate Gradient to
generate templates in real time and enrich rectangular boxes with high
quality 2D-3D matching. The 2D-3D matching provides a CAD model,
sub-category, rendering, depth, viewpoint and focal length.

Our method can work as a standalone 2D-3D matching algorithm, or even
as a standalone ensemble of exemplar-based object detectors (Figure
\ref{fig:front}) but its real power comes from combining generic
object detectors and complement rectangular bounding boxes. Our method
is simple in design; is several orders of magnitude faster than
traditional method; requires only CAD models; does not need neither images nor
calibration, yet is powerful in giving high-quality metadata. These
properties make our method ideal for complementing object detections. 

To complement detection bounding boxes, we first search possible 2D-3D
matching by running cached discrete viewpoint templates. Then, for the
most probable match, we refine the initial matching using Metropolis
Hastings. Since it requires iterations of proposal and validation, we
present real time analysis of NZ-WHO template generation to show that
this can be done on-the-fly. The fine tuning stage process jointly
optimizes translation, yaw, roll, pitch, scale, and focal length
continuously and also searches for different CAD models.

\end{comment}
\subsection{Rendering Pipeline}

\subsection{Non-Zero Whitened Histograms of Orientations (NZ-WHO)}

Recently, Hariharan \etal. introduced Whitened Histograms of Orientations (WHO)
\cite{Hariharan12}, which uses statistics from natural images to whiten HOG
templates, making them more discriminative. Whitening is a common signal
processing operation for decorrelating a set of random variables
\cite{Martinsson05, Belouchrani00}. More formally, suppose that we have a
$k$-dimensional random variable $X \in \mathcal{R}^k$ with $Cov(X)=\Sigma$. By
whitening the signal,
\begin{equation}
\tilde{X}=\Sigma^{-\frac{1}{2}}(X - E[X]) \label{eq:whitening}
\end{equation}
, we
remove 2nd order correlation between all features. So, all elements in the
random variable $\tilde{X} = \left[\tilde{X}_1, \tilde{X}_2, \dots,
\tilde{X}_k\right]$, become more independent. In the context of HOG features, a
whitened HOG template will produce Gaussian noise when convolved with a random
natural image, but a delta-like response on an image matching the template. WHO
templates are more discriminative than standard HOG templates because they only
capture the gradients that differentiate the object from random images.

Hariharan \etal. interpreted the
whitening as Linear Discriminant Analysis (LDA) where all classes have different
means but have the same covariance. The whitening and LDA have the same
formulation when we make a decision boundary $w = \Sigma^{-1}(\mu_+ - \mu_0)$
where $\mu_+$ is the features from a class (LDA) or a signal to whiten
(whitening), $\mu_0$ is the negative class center (LDA) or the signal mean
(whitening) and $\Sigma$ is the covariance matrix of classes (LDA) or
the correlation within signal (whitening).

We propose Non-Zero Whitened Histograms of Orientations (NZ-WHO), that are both more discriminative than WHO, and can be generated several orders of magnitude faster (in 70ms compared to several seconds for WHO). This speed up means we can generate templates on the fly, allowing our approach to evaluate arbitrary viewpoint hypotheses.

Table \ref{tab:who_initializations} provides a detailed comparison of WHO and NZ-WHO templates, illustrating that our method is faster and more discriminative than Whitened Histograms of Orientations. \scream{Does the table show this?}

\subsubsection{Generating $\Sigma$ and $\mu$}
\scream{this section seems important to understand the following section but it's also just a re-writing of the Hariharan method}
In \cite{Hariharan12}, the authors compute the mean and covariance matrix by assuming Wide-Sense Stationarity (WSS) of HOG features
generated from natural images. That is, they assume the mean of a HOG cell is
independent of its place in the image, and the autocovariance of cells depends
only on their relative location.

In one dimension, let $x(u)$ be the feature at location $u$. Then WSS states
that $\mathbb{E}\left[x(u)\right] = \mu$ for all $u$, and
\begin{equation}
\textrm{cov}_x(u,v) = \textrm{cov}_x(0, v-u) = \Gamma(v-u),
\end{equation} where $\Gamma$ is the autocovariance. For simplicity, we describe the 1D case but this can be easily extended to 2D spatial autocovariance.

Therefore, assuming WSS allows the covariance matrix of templates \emph{of any size} to be synthesized from the autocovariance matrix using a simple lookup.

\subsubsection{Whitening Synthesized Templates}
Our first innovation is `Non-Zero' whitening. When synthesizing detection templates from rendered images, a common problem is how to handle the background. If the model is rendered over a natural image background, gradients in the background will be incorporated into the discriminative template, potentially causing false-positives if background elements exist in the query image.

Alternatively, if the background is left textureless \scream{did Hariharan do this?} (see Fig.~\ref{fig:rendering}), whitening the resulting HOG template will produce undesirable background artifacts. These are created when centering the template (by subtracting the mean $\mu$), where strong negative weights are introduced in the textureless region (as seen in Fig.~\ref{fig:whocomparison}). This could result in positive matches being suppressed due to spurious background gradients.

\scream{might need to match notation with subsequent sections}
NZ-WHO removes these artifacts so that the background has no effect on the template response. Given HOG features $x$, we create a new vector $x'$ which contains only the non-zero elements of $x$. We select the same elements of $\mu$ to produce $\mu'$, while the autocovariance lookup is only performed for the cells with non-zero gradient, producing $\Sigma'$ corresponding to the features in $x'$. After solving the resulting system (which is now smaller than in the WHO approach) we find
\begin{equation}
w'=\Sigma'^{-1}(x' - \mu') \label{eq:nz-who},
\end{equation}which we reassemble into the final NZ-WHO template $w$ by replacing the non-zero elements of $x$ with the corresponding $w'$ element.

Notice that $\Sigma$ is not square-rooted in Eq.~\ref{eq:nz-who}, whereas it is in the definition of whitening (Eq.~\ref{eq:whitening}). This is because the whitened template will only be used in the context of detection, which involves the inner product with the whitened HOG features of the query image $y$:
\begin{equation}
(y-\mu)^{T}\Sigma^{-\frac{1}{2}}\Sigma^{-\frac{1}{2}}(x-\mu)\\
= (y-\mu)^{T} w.
\end{equation}
We can therefore capture the whitening of the query image features in $w$.

% We use the GPU to parallelize the lookup of the correct autocovariance cells during covariance matrix syntheses, which is \scream{x} times faster than using the CPU (Figure \ref{fig:covariancetime}).


%\scream{chris:We propose Non-Zero Whitened Histograms of Orientations (NZ-WHO), an improvement over WHO which is both more discriminative than WHO and can be computed many orders of magnitude faster than WHO (70ms as opposed to to several seconds for WHO). This speedup allows us to generate NZ-WHO templates on the fly, allowing us to quickly evaluate arbitrary viewpoint hypotheses.}

\subsubsection{Fast Whitening using Conjugate Gradient}

The computational efficiency of our approach comes from using the iterative conjugate gradient method to whiten the HOG template. Whitening the non-zero feature vector $x'$ requires a system of linear equations $\Sigma' w' = (x' - \mu')$ to be solved. In
\cite{Hariharan12}, the authors make use of the fact that covariance matrices
are symmetric and positive semidefinite to solve the system via the Cholesky
decomposition, which requires $O(n^3)$ time.

The conjugate gradient method is an iterative algorithm for solving symmetric positive definite systems which runs in $O(n^2\kappa)$ time, where $\kappa$ is the condition number of the matrix.
This makes conjugate gradient faster than decomposition for matrices with small condition
numbers relative to their size.

The covariance matrix for HOG templates is typically ill-conditioned, but we can add a regularization constant to the diagonal to reduce its condition number to the point where the conjugate gradient method will converge quickly. We use a constant of $0.15$, which reduces the condition number from $10^{20}$ to $50$, much smaller than the dimension of the matrix (7000).

As a result, a GPU implementation of conjugate gradient converges in \scream{x
ms}, two orders of magnitude faster than the system can be solved using the
Cholesky factorization (\scream{5 s}).

\subsubsection{Fast Convolution using the FFT}






% We compare the performance of WHO and NZ-WHO on 3D Object dataset \cite{Savarese07} and report performance on Table \ref{tab:who_initializations}. 

% To deal with the problems, we propose Non-Zero Whitened Histograms of Orientations.

% To accommodate the growing usage of rendering image for 2D-3D matching using
% WHO\cite{Aubry13, Aubry14, Lim14}, we analyzed various methods to whiten
% synthetic rendering image and propose Non-Zero Whitened Histograms of
% Orientations (NZ-WHO) which is a direct extension of \cite{Hariharan12}
% designed for rendering images with textureless background. % Since a NZ-WHO
% template has zero-mean and all elements in the template follow same
% distribution, no explicit calibration is required if all the templates have
% approximately the same number of cells. 

% Since Combining NZ-WHO with Conjugate Gradient, 

% \subsection{Non-Zero Whitened Histograms of Orientations (NZ-WHO)}
% why non zero whitening and why it makes more sense than seeing 3D, IKEA
% Overview of the subsections
%   Generating covariance quickly
%   Fast inversion
%   Regularization

\begin{figure}[t]
  \begin{center}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    % include whitening all centered cells
    % \includegraphics[width=0.32\linewidth]{whiten_all_crop}
    \setlength\tabcolsep{3pt}
    \begin{tabular}{ccc}
      HOG & WHO & NZ-WHO \\
%     \begin{turn}{90}$w_+$\end{turn} &
    \includegraphics[width=0.28\linewidth]{hog_crop} &
    \includegraphics[width=0.28\linewidth]{whiten_all_crop} &
    \includegraphics[width=0.28\linewidth]{whiten_non_zero_crop} \\
    % include whitening all centered cells
     % \includegraphics[width=0.282\linewidth]{whiten_all_neg_crop} 
%     \begin{turn}{90}$-w_-$\end{turn} &
     \includegraphics[width=0.28\linewidth]{hog_neg_crop} &
     \includegraphics[width=0.28\linewidth]{whiten_all_neg_crop}  &
     \includegraphics[width=0.28\linewidth]{whiten_non_zero_neg} \\
    % include whitening all centered cells
 %    \begin{turn}{90}ihog\cite{vondrick2013}\end{turn} &
    % \cite{vondrick2013}& 
     \includegraphics[width=0.28\linewidth]{ihog_hog200_crop.png} &
     \includegraphics[width=0.28\linewidth]{ihog_whiten_all200_crop.png} &
     \includegraphics[width=0.28\linewidth]{ihog_whiten_non_zero200_crop.png} \\
 \end{tabular}
  \end{center}
  \caption{Comparison of HOG, WHO and NZ-WHO. Visualization of positive weights (first row),  visualization of negative weights (second row), HOGgles \cite{vondrick2013} (third row). Note that for WHO, whitening all cell result in strong negative edges on the empty region}
  \label{fig:whocomparison}
\end{figure}

% Overview of the subsections
% In this section, we will give a brief overview of whole whitening stages. In the first section \ref{feature_statistics}, we will discuss about first and second order statistics of HOG feature. Then, we will discuss about using autocovariance $\Gamma$ to create covariance matrix $\Sigma$ for non-zero cells and SIMD implementation to speed up the generation. Then, we will explain fast and accurate way to solve $w = \Sigma^{-1}x$ using Conjugate Gradient method. % Finally, we will analyze the effect of regularization.

% \subsection{Whitening and Textureless Background}
\begin{comment}
\subsection{Constructing the Covariance Matrix}
\label{sec:feature_statistics}
% Whats used in our work and using Autocovariance matrix
We first collected the first and second order statistics (mean and
autocovariance) of HOG features from arbitrary natural images. Following
\cite{Hariharan12}, we assume Wide-Sense Stationarity (WSS) of HOG features
generated from natural images. That is, we assume the mean of a HOG cell is
independent of its place in the image, and the autocovariance of patches depends
only on their relative location.

In one dimension, let $x(u)$ be the feature at location $u$. Then WSS states
that $\mathbb{E}\left[x(u)\right] = \mu_x$ for all $u$, and
\begin{equation}
\textrm{cov}_x(u,v) = \textrm{cov}_x(0, v-u) = \Gamma(v-u),
\end{equation} where $\Gamma$ is the autocovariance. For simplicity, we show the 1D case but this can be easily extended to 2D spatial autocovariance.

Therefore, assuming WSS allows us to synthesize the covariance matrix for
templates \emph{of any size} from the autocovariance matrix. This hugely reduces
the memory required because to build the covariance matrix of a template of $w
\times h$ HOG cells (with 31 elements per cell), we only need a $w \times h
\times 31$ autocovariance matrix. In comparison, the covariance matrix itself is
$(w \times h \times 31) \times (w \times h \times 31)$. Furthermore, by using
the autocovariance we can avoid storing the covariance matrix for every
different aspect ratio used during detection.

To further simplify $\Gamma$, we assumed horizontal and vertical symmetry. In
our implementation, we compute a $40 \times 40 \times 31$ autocovariance matrix,
meaning we can synthesize covariance matrices for HOG grids as big as $40 \times
40$. In practice we limit the number of cells per template to 250.

We use the GPU to parallelize the lookup of the correct autocovariance cells
during covariance matrix syntheses, which is \scream{x} times faster than using
the CPU (Figure \ref{fig:covariancetime}).

\end{comment}

% Since the statistics have been computed for a generic HOG feature, we can use
% the same statistics for all templates and images. In practice, since it is
% expensive to whiten all the sliding patches in the image, we followed
% \cite{Hariharan12} and defined the templates to be $w =
% \Sigma^{-1}(X-E[X])$\begin{comment}which is a good approximation of
% $\Sigma^{-\frac{T}{2}} \Sigma^{-\frac{1}{2}}(X-E[X])$\end{comment} 

% \begin{comment}Gathering statistics, especially computing spatial covariance
% of a signal is computationally burdensome. \end{comment} To speed up the
% statistics collection, \begin{comment}in addition to the wide-sense
% stationarity (WSS) of HOG, \end{comment}we assumed horizontal and vertical
% symmetry of covariance. 

 %Hariharan \etal \cite{Hariharan12} interpreted the whitening also as the
 %Linear Discriminant Analysis where each class has the same covariance $\Sigma$
 %but with different mean value. They defined this whitened HOG feature as
 %Whitened Histograms of Orientations (WHO) and we will follow their convention
 %of the name.

% \subsection{Rendering}
% 
% We used a public rendering engine to create realistic rendering and
% depth\begin{comment}\cite{Choy14render}\end{comment}. The CAD models we used
% in our experiments contain color, texture and material information such as
% transparency and reflectance. We tried to make the rendering as much realistic
% as possible to simulate the natural image statistics. The example rendering is
% in Fig. \ref{fig:rendering}. We also extracted depth information as well while
% rendering to use in the reconstruction.


% To whiten a HOG features, we first have to synthesize the covariance matrix.
% Following the method of \cite{Hariharan12}, which assumes spatial Wide-Sense
% Stationarity (WSS) of a feature, covariance matrix can be synthesized using
% spatial autocovariance $\Gamma$. This assumes the \begin{comment} and features
% such as HOG are gradients in a small patch whose statistics does not change
% (an object can be in anywhere in an image)\end{comment}. 

%However, unlike small fixed-sized mid-level patches, the root templates have
%different aspect ratios, different gradient patterns and large number of HOG
%cells which results in large covariance $\Sigma \in \mathcal{R}^{7000 \times
%7000}$ that is difficult to cache. Thus, to generate a NZ-WHO template, we have
%to synthesize a covariance matrix and solve the system of linear equations. 



% Thus, every time we whiten a root template, we have to generate a covariance
% matrix using autocovariance $\Gamma$. 

%\begin{comment} Also, since we are whitening only the non-zero HOG cells whose
%position is unknown beforehand and all templates have different aspect ratio
%for , we cannot cache the matrix. Using CPU, it takes about \end{comment}

% In practice, synthesizing covariance matrix $\Sigma$ takes long time due to
% its large size.  $\Sigma$ ranges from $\Sigma \in \mathcal{R}^{7000 \times
% 7000}$ to $\Sigma \in \mathcal{R}^{7500 \times 7500}$. We vary the template
% resolution and run $\Sigma$ generation on CPU and GPU 100 times and report the
% average time in Fig. \ref{fig:covariancetime}.

\begin{figure}[t]
  \begin{center}
     \includegraphics[width=0.9\linewidth]{covariancetime} 
  \end{center}
  \caption{Time to generate $\Sigma$ for various number of cells (left) and same plot on log scale(right). The time complexity increases as $O(n^2)$ for $n$ number of cells.}
  \label{fig:covariancetime_crop}
\end{figure}
 
% 1-dimensional version of SIMD implementation that generate covariance matrix $\Sigma$ from autocovariance $\Gamma$ on GPU is on the supplementary material.
% \begin{comment}i
% \begin{algorithm}
% \KwData{ $\Gamma$ , nonzero cell indexes, thread x id, thread y id}
% \KwResult{ $\Sigma$ }
% \Begin{
% $c_u \leftarrow \mod(x, N_f)$ define cell $u$ coordinate\; 
% $f_u \leftarrow u / N_f$ define feature index
% $c_v \leftarrow \mod(y, N_f)$ define cell $v$ coordinate\;
% $f_v \leftarrow v / N_f$ define feature index
% $T \leftarrow $ nonzero cell indexes\;
% $\Sigma(w) \leftarrow \Gamma( N_f ( T(u) - T(v) +  )$\;
% }
% \caption{SIMD implementation of covariance synthesis from autocovariance}
% \end{algorithm}
% \end{comment}



The final challenge in on-the-fly template evaluation is performing the
convolution between the template and the target image. Standard convolution
would be prohibitively slow for the high resolution templates we're using, so we
implemented the convolution via Fast Fourier Transform on the GPU. This allows
template evaluation to run in \scream{x ms}, compared to \scream{x ms} for the
naive CPU implementation.

  
%  If we use a high resolution template such as \ref{fig:rendering}

% where $w$ is the weight we want to find and $x$ is the HOG feature from the
% rendering and $\lambda$ is the regularization constant (see section
% \ref{sec:regularization}). For $w\in \mathcal{R}^n$, it takes $O(n^3)$ time to
% solve the linear equation. If we use a high resolution template to capture
% small geometric properties we can get high accuracy matchings but the solving
% the linear equation becomes computationally burdensome. 

% To briefly recap, the LU Decomposition and Cholesky Decomposition are most
% popular methods to solve a linear equation. In linear algebra libraries such
% as MATLAB, these decomposition methods are default method to solve a linear
% equation (backslash \textbackslash in MATLAB). Since the covariance matrix is
% Hermitian and Positive Semi-Definite, we can use Cholesky Decomposition which
% is twice as fast as the LU Decomposition.  

% decomposition methods such as LU or Cholesky Decomposition not only take long
% time but they also accumulate numerical error. 

%In our setting, we used high resolution template which has more than 250 HOG
%cells. This result in $\Sigma \in \mathcal{R}^{7500 \times 7500}$. It takes
%about 9 seconds if we use LU decomposition and 5 seconds if we use Cholesky
%decomposition.  % The default way of solving the linear equation in many linear
%algebra library is decomposing the matrix using LU Decomposition (in MATLAB the
%default method of backslash operator) which takes $O(\frac{2}{3}n^3)$ flops for
%$n\times n$ matrix. Since the matrix, $\Sigma + \lambda I$ is Hermitian and
%Positive Semi Definite. We can speed up using Cholesky Decomposition. 



%We also used iterative Conjugate Gradient to make use of low rank property of
%the covariance \cite{Gharbi12}. After adding small regularization, the
%regularized covariance matrix has nice spectral property (Figure
%\ref{fig:spectral}). The singular values are clustered in small region and most
%of the energies are concentrated on the first (number) singular values. Thus,
%the conjugate gradients stage doesn't go upto the size of the matrix to
%converge.

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{cc}
     \includegraphics[width=0.5\linewidth]{whotime} & 
     \includegraphics[width=0.5\linewidth]{speedup}\\
     (a) & (b) \\
 \end{tabular}
  \end{center}
  \caption{Comparison of HOG variants generation time and final }
  \label{fig:whotime}
\end{figure}
[Figure spectral analysis]

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{residual}
  \caption{Residual of differentd method}
  \label{fig:whotime}
\end{figure}

\subsection{Variations of WHO}
\scream{sam thinks this should be moved to the experiments section}
We empirically found out that NZ-WHO performs reasonably well without time
consuming calibration stage compare to other variational method to generate
templates. We used 1 CAD model with viewpoints covering 24 azimuths and 4
elevations. Each template takes approximately 80 milliseconds to generate a
NZ-WHO template. We also calibrated templates using the method presented in
\cite{Aubry14}. The calibration learns affine transformation of the detection
confidence.% Note that in Section \ref{experiments}, we provide the same
experiment with 9 CAD models.


\begin{table*}[!htbp]
    \footnotesize
    \begin{center}
\begin{tabular}{|c|c|r|c|r|}
\hline
Methods (AP/MPPE) & before calibration  & time & after calibration \cite{Aubry14} & time \\
\hline\hline
HOG\cite{Dalal05}     & 72.3 / 65.0           &  31ms  & 60.4 / 50.2                 & 8.7 sec \\ 
WHO\cite{Hariharan12} & 82.1 / 85.4           &  3811ms& 84.4 / 83.0                 & 12.4 sec  \\
WHO-CG                & 81.7 / \textbf{84.9}  &  104ms & 83.7 / 87.3                 & 8.3 sec \\
WHO-CG-Z              & 54.4 / 65.1           &  103ms & \textbf{92.8} / 86.7        & 8.7 sec  \\
% NZC-WHO-Z    & 89.10/\textbf{78.64} &    & 91.15/74.79                  &     \\
NZ-WHO-CG             & \textbf{90.0} / 82.8  &   79ms & 90.3 / \textbf{86.8}        & 8.5 sec   \\
\hline
\end{tabular}
\end{center}
\caption{Average Precision(AP) and Mean Precision in Pose Estimation (MPPE) \cite{Lopez-Sastre11} variations of WHO on 3DObject Car dataset\cite{Savarese07}. WHO refers to standard WHO using the method presented in \cite{Hariharan12}, WHO-CG uses iterative Conjugate Gradient method to generate WHO. WHO-CG-Z uses whiten the whole template and zero out textureless region. NZ-WHO-CG is the NZ-WHO which whitens only non-zero cells using iterative Conjugate Gradient method. The time column indicates the time to generate one template. We followed calibration procedure presented in \cite{Aubry14}.}
\label{tab:who_initializations}
\end{table*}

We found out that without calibration, NZ-WHO performs the best on object
detection on 3DObject car dataset \cite{Savarese07}. See Table
\ref{tab:who_initializations} for detail. In essence, if we use calibration, we
could achieve better performance but since our goal is to do 2D-3D matching and
continuous viewpoint estimation using on-the-fly template generation, we did not
calibrated our templates.


\begin{comment}
\subsection{Whitening Non-Zero Cells}

% learning weights for whitened templates
But since it is difficult to make templates to have the number of cells, \cite{Hariharan12, Aubry14, Lim14} learned weights for convolution score from the template. However, we propose a novel way to guarantee normalized score distribution by decorrelating non-zero HOG cells and forcing each template to have approximately same number of cells.

% 
Unlike \cite{Aubry14}, we did not whiten all cells and then zero out some HOG cells that are on the background. Instead, we whiten only the cells with non-zero value. We empirically found out that by whitening the cells that has non-zero support actually performs better and takes less time solving the linear equation $w = \Sigma^{-1}x$ since the number of elements to whiten decreases. However, by selecting non-zero cells, we destroys inherent Toeplitz-Block-Toeplitz property of the covariance matrix which could speed up the inversion using Fourier Transformation \cite{Akaike73, Martinsson05}.

  We propose a whitening stage for WHO feature template generation that can create template whose convolution score follow same distribution. This guarantees that we do not require any calibration stage after generating templates rapidly. We statistically compare the score distribution of whitening non-zero HOG cell only and whitening.
  \end{comment}


% \subsection{Effect of Regularization}
% \label{sec:regularization}
%  Also, since all covariance matrix is Positive Semi Definite, our matrix
%  $\Sigma$ must be Positive Semi-Definite too. To guarantee the property, We
%  found out that the regularization has significant impact on the performance.
%  In \cite{Hariharan12}, Hariharan \etal used small fixed regularization since
%  the covariance matrix of specially correlated features is intrinsically
%  row-rank to make the matrix invertible.
%  
% However if we closely analyze the impact of the regularization, we can see that larger the $\lambda$ is, the centered HOG becomes more dominant. 
% 
% \begin{align}
% (\Sigma + \lambda I)^{-1} & = \frac{1}{\lambda}\left(I + \frac{1}{\lambda}\Sigma \right)^{-1} \\
% & = \frac{1}{\lambda}\left(I - \frac{1}{\lambda} \Sigma  + \frac{1}{\lambda^2} \Sigma^{2} 
% 		- \frac{1}{\lambda^3} \Sigma^{3} + \cdots \right)
% % & = \frac{1}{\lambda}I - \left(I + \frac{1}{\lambda} \Sigma \right)^{-1} \frac{1}{\lambda^2} \Sigma
% % & = \frac{1}{\lambda}\left(I  - \frac{1}{1 + \frac{1}{\Sigma} \Tr{\Sigma}} \right)
% \end{align} 
% 
% We empirically found out that as $\lambda$ increases, AP converges to that of centered HOG feature without any whitening.

\begin{comment}
\subsection{No Calibration}
  We created coarse viewpoint templates by whitening non-zero HOG cells and compare the average precision on 3D Object dataset \cite{Savarese07} with that of whitening all HOG cells. 
  
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Whitening and AP & Whiten & No Whitening \\
\hline\hline
Average Precision & 89.205 & 74.478 \\
\hline
\end{tabular}
\end{center}
\caption{Effect of whitening on 3D Object Car validation set using one CAD model. The whitening dramatically increased the AP.}
\end{table}
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Calibration methods \&AP & Whiten non-zero & Whiten all \\
\hline\hline
None & 89.205 & 86.676 \\
Linear & 88.430 & 85.720 \\
Gaussian & 87.913 & 84.567\\
Number of non-zero cells & 84.734 & N/A\\
\hline
\end{tabular}
\end{center}
\caption{Various calibration and AP on small experimental set. Since the whitening templates with constant number of HOG cells create normalized feature, calibration using negative patches did not improve the AP.}
\end{table}
\end{comment}

\begin{comment}
\subsection{High Resolution Templates and FFT based Convolution}
\label{sec:fft} 
We generate high resolution templates with more than 250 HOG cells to capture details of an object to give accurate 2D-3D matching. These large templates cause computational burden when computing convolution. Though good for detecting accurate model and pose, convolution of high-resolution templates are much more slower since computation time scales linearly with the number of HOG cells in the template. To overcome and speed up the convolution, we propose FFT based GPU convolution which scales. Suppose there are length $n$ signal and length $m$ filter, naive convolution takes $O(nm)$ time where as FFT-based convolution takes $O\left( (n + m)\log (n+m) \right)$ time. For large $m$, high resolution template, we can gain computational advantage.


we pad zero to the 2D image features by the largest kernel size. And we transform the kernel to match the padded image feature. To compute convolution, we compute element-wise product with the transformed kernel and sum all the features. Unlike \cite{Podlozhnyuk}, we do not need to shift the data nor kernel if we use simple mathematical trick. 
  
\begin{align}
    x_{pad} & = \mathcal{P}_{n+m}(x)\\
    y_{pad} & = \mathcal{P}_{n+m}(y)\\
    x_{pad} \ast y_{pad} & = \mathcal{F}^{-1}(\mathcal{F}(x_{pad}) \circ \mathcal{F}(y_{pad}))
\end{align}

  Where $\ast$ is the circular convolution and $\circ$ is the Hadamard Product and $\mathcal{F}$ is the Fourier Transformation and $\mathcal{P}_n$ is the padding operation that append zeros to make vector of size $n$.

For each dimension of features, we transform kernel and image feature into frequency domain using Fast Fourier Transformation. We transform the image into frequency domain once and use the transformed image for all templates.

We implemented the convolution using FFT on GPU and got 10x speed up.

[Figure for convolution time for different template resolution, CPU, GPU, naive convolution]
\end{comment}
%------------------------------------------------------------------------

\section{Estimating Parameters using MCMC}
\label{sec:fine}
% Estimating viewpoint continuously to fine tune a initial matching
The NZ-WHO template matching method we have presented makes template generation
and evaluation computationally inexpensive. This means we can efficiently
explore the parameter space to find the best object pose, scale, model type and
camera focal length.

Our method begins by convolving the 

Since creating a template and validating the template become computationally
inexpensive, making a proposal and validating the proposal to fine tune a 2D-3D
matching became affordable. We used Metropolis-Hastings algorithm to generate a
proposal pose and focal length and validate the proposal using the convolution
and returning maximum response as a return.

To fine-tune continuous pose and location of object, we require strong
initialization. The initialization is a rough bounding box with small context
and viewpoint. Since most of the standard detectors does not provide viewpoint
of an object, we first run an ensemble of NZ-WHO templates to get rough
viewpoints. Once we get rough viewpoint, we initialize the proposal $x_0$ and
start sampling proposals.

We modeled the distribution of object location and pose as an exponential distribution. 
For proposal distribution, we used Gaussian distribution to propose next viewpoint and for viewpoint proposals
and uniform distribution for model change proposals.  

More formally, suppose that $x = [p, m, f]$ where $p$ is the 3D rotation of the CAD model and
$m$ is the CAD model index and $f$ is the focal length. We use Metropolis Hastings
to optimize $x$.

\begin{align}
    A(x \rightarrow x') & =  min\left( 1,  \frac{P(x' g(x') \rightarrow x)}{P(x) g(x \rightarrow x')}\right) \\
    g(x \rightarrow x + p_i') & \sim \mathcal{N}(\mu = 0,\sigma = c_1) \quad i \in \{1,2,3\}\\
    g(x \rightarrow x + f') & \sim \mathcal{N}(\mu = 0,\sigma = c_2)\\
    g(x \rightarrow x + m') & \sim c_3 \delta(m) + (1-c_3) Unif(1,M)
\end{align}


Where $g$ is the proposal distribution and $P$ is the distribution of which we
defined using Gaussian distribution around previous state. $A$ is the
acceptance probability where we accept new proposal with $A(x \rightarrow x')$.

\begin{align}
    P(x) & \sim e^{ c_4 \max_{s} w(x) \ast \mathcal{T}_s(\mathcal{I}) }
\end{align}

$P$ is the distribution of the pose and location of an object on image
$\mathcal{I}$. One can also think of the image as an image cropped by detection
bounding box. We synthesize covariance matrix and create NZ-WHO template $w(x)$ for particular viewpoint, focal length and CAD model.

We initialize the $x$ using the result from the previous stage where we run our ensemble or NZ-WHO templates to get rough estimate.

In sum, for new proposal $x'$, we make a NZ-WHO template on the fly 
We found out that it requires good initialization to converge to local minima. 

We represented the probability using exponential family. 

So our model favors large convolution score $\max_{s} w(x) \ast T(\mathcal{I})$. 
% there are several advantages that was not possible such as \textbf{on-the-fly} template generation and making large number of templates  
\begin{figure}[t]
\centering
    \includegraphics[width=0.7\linewidth]{tuning2} \\ [-5pt]
    \caption{Different modes of Metropolis Hasting proposals, see Section \ref{sec:fine}}
 \label{fig:tuningmode}
\end{figure}
    
\begin{figure}[t]
 \begin{center}
    \setlength\tabcolsep{0pt}
    \begin{tabular}{ccc}
    % \includegraphics[width=0.9\linewidth]{tuning} 
   \includegraphics[width=0.33\linewidth]{tuning/1.png} &
   \includegraphics[width=0.33\linewidth]{tuning/2.png} &
   \includegraphics[width=0.33\linewidth]{tuning/3.png} \\[-5pt]
   (a) & (b) & (c)\\
   \includegraphics[width=0.33\linewidth]{tuning/4.png} &
   \includegraphics[width=0.33\linewidth]{tuning/5.png} &
   \includegraphics[width=0.33\linewidth]{tuning/6.png} \\[-5pt]
   (d) & (e) & (f)\\
   \end{tabular}
 \end{center}
 \caption{Effect of fine tuning. (left) original image, (middle) initial detection, (right) continuous fine tuning}
 \label{fig:tuning}
\end{figure}



\section{Experiments}
\label{sec:experiments}
\input{experiments}



\section{Conclusion}


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
