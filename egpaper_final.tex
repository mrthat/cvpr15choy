\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{algorithm2e}
\usepackage{rotating}
\usepackage[space]{grffile}
\usepackage[font=small,skip=0pt]{caption}
%\DeclareMathOperator{\Tr}{Tr}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\graphicspath{ {figures/} }
% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% custom commands
\newcommand{\scream}[1]{{\color{red} \bf *** #1 ***}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint}
%\title{Enrich Object Detection : 2D-3D registration and continuous viewpoint estimation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
A large body of recent work on object detection has focused on exploiting 3D CAD
model databases to improve detection performance. Many of these approaches work
by aligning exact 3D models to images using templates generated from renderings
of the 3D models at a finite set of discrete viewpoints. The training procedures
for these approaches, however, are very expensive and require gigabytes of
memory and storage, and the viewpoint discretization hampers pose estimation
performance.

We propose an efficient method for synthesizing templates from 3D models that
runs on the fly -- that is, it quickly produces detectors for an arbitrary
viewpoint of a 3D model without expensive dataset-dependent training or template
storage. Given a 3D model and an arbitrary continuous detection viewpoint, our
method synthesizes a discriminative template by extracting features from a
rendered view of the object and decorrelating spatial dependences among the
features. Our decorrelation procedure relies on a gradient-based algorithm that
is more numerically stable than standard decomposition-based procedures, and we
efficiently search for candidate detections by computing FFT-based template
convolutions. Due to the speed of our template synthesis procedure, we are able
to perform joint optimization on continuous scale, translation, rotation, and
focal length. We provide an efficient GPU implementation of our algorithm, and
we validate its performance on 3DObject dataset and PASCAL 2012 dataset.

%     Using 3D model to detect and register the model to RGB image has been a
%     growing field of study. Recently, Aubry \etal \cite{Aubry14} and J. Lim
%     \etal \cite{Lim14} tried to estimate pose and align exact CAD model to an
%     image using part-based templates. Malisiewicz \etal \cite{Malisiewicz11}
%     also addresses the issue by transferring metadata after detection. Hejrati
%     \etal \cite{Hejrati14} uses template synthesis to detect an object. All
%     these approaches require extensive training, a lot of memory space. Also,
%     the training based approach requires defining fixed viewpoints which limit
%     themselves.  To overcome these difficulties, we propose an efficient and
%     fast way to synthesize and validate templates using on the fly realistic
%     rendering that can jointly optimize scale, translation, rotation and focal
%     length continuously. We render an object and extract features and
%     decorrelate spatial dependencies within them on the fly to make a
%     discriminative template. To speed up the decorrelation procedure, we adopt
%     a gradient based algorithm which is numerically more stable than
%     decomposition procedure. Finally, we efficiently search for candidate
%     match by computing convolution using FFT and jointly optimize to match the
%     object accurately. To speed up further, we propose an efficient GPU
%     implementation and tested on PASCAL images.

%   In contrast to other mid-level patch based 3D matching methods which
%   requires extensive training and more than 10Gb of memory space and physical
%   storage, ours require no training nor saving the templates thus enabling it
%   to run on any personal computer.  To overcome these difficulties, we propose
%   an efficient and fast way to synthesize and validate templates using on the
%   fly realistic rendering that can jointly optimize scale, translation,
%   rotation and focal length continuously. We render an object and extract
%   features and decorrelate spatial dependencies within them on the fly to make
%   a discriminative template. To speed up the decorrelation procedure, we adopt
%   a gradient based algorithm which is numerically more stable than
%   decomposition procedure. Finally, we efficiently search for candidate match
%   by computing convolution using FFT and jointly optimize to match the object
%   accurately. To speed up further, we propose an efficient GPU implementation
%   and tested on PASCAL images.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{intro}
%
\section{Related Work}
\label{sec:related}
\input{related}


\section{Approach Overview}
\label{sec:nz-who}
\input{overview}

\section{Pipeline}
\subsection{Rendering}

We used a public rendering engine to create realistic rendering and
depth. We used CAD models that contain color, texture and material information such as
transparency and reflectance to make the rendering as much realistic
as possible to simulate the natural image statistics. The example rendering is
in Fig. \ref{fig:rendering}. We also extracted depth information as well while
rendering to use in the reconstruction.

We used a collection of CAD models to make exemplar renderings of CAD models and for viewpoints. The
CAD models are cached so that when we fine-tune 2D-3D matching, so it can be rendered instantaneously.
We made the renderings to change focal length and yaw, pitch, roll so that we can fine tune the matching
continuously.

\begin{figure}[t]
  \begin{center}
     \includegraphics[width=0.4\linewidth]{rendering}
     \includegraphics[width=0.4\linewidth]{depth}
  \end{center}
  \caption{Example rendering and depth made by our renderer}
  \label{fig:rendering}
\end{figure}

\subsection{Whitened Histograms of Orientations (WHO)}

Recently, Hariharan \etal. introduced Whitened Histograms of Orientations (WHO)
\cite{Hariharan12}, which uses statistics from natural images to whiten HOG
templates, making them more discriminative. Whitening is a common signal
processing operation for decorrelating a set of random variables
\cite{Martinsson05, Belouchrani00}. More formally, suppose that we have a
$k$-dimensional random variable $X \in \mathcal{R}^k$ with $Cov(X)=\Sigma$. By
whitening the signal,
\begin{equation}
\tilde{X}=\Sigma^{-\frac{1}{2}}(X - E[X]) \label{eq:whitening}
\end{equation}
, we remove 2nd order correlation between all features. Since collecting covariance matrix for every template shape
 is expensive, \cite{Hariharan12} proposed easy way to synthesize
covariance $\Sigma$ from autocovariance $\Gamma$ and we followed their method to generate $\Sigma$

% The whitening and LDA have the same
% formulation when we make a decision boundary $w = \Sigma^{-1}(\mu_+ - \mu_0)$
% where $\mu_+$ is the features from a class (LDA) or a signal to whiten
% (whitening), $\mu_0$ is the negative class center (LDA) or the signal mean
% (whitening) and $\Sigma$ is the covariance matrix of classes (LDA) or
% the correlation within signal (whitening).

% In \cite{Hariharan12}, the authors compute the mean and covariance matrix by assuming Wide-Sense Stationarity (WSS) of HOG features
% generated from natural images. That is, they assume the mean of a HOG cell is
% independent of its place in the image, and the autocovariance of cells depends
% only on their relative location.

% In one dimension, let $x(u)$ be the feature at location $u$. Then WSS states
% that $\mathbb{E}\left[x(u)\right] = \mu$ for all $u$, and
% \begin{equation}
% \textrm{cov}_x(u,v) = \textrm{cov}_x(0, v-u) = \Gamma(v-u),
% \end{equation} where $\Gamma$ is the autocovariance. For simplicity, we describe the 1D case but this can be easily extended to 2D spatial autocovariance.
% 
% Therefore, assuming WSS allows the covariance matrix of templates \emph{of any
% size} to be synthesized from the autocovariance matrix using a simple lookup.

% However, the 
% We propose Non-Zero Whitened Histograms of Orientations (NZ-WHO), that are both
% more discriminative than WHO, and can be generated several orders of magnitude
% faster (in 70ms compared to several seconds for WHO). This speed up means we
% can generate templates on the fly, allowing our approach to evaluate arbitrary
% viewpoint hypotheses.

% Our method requires generating a covariance matrix of non-zero cells 

\subsubsection{Whitening Synthesized Templates and Non-Zero WHO}
Our first innovation is `Non-Zero' whitening. When synthesizing detection
templates from rendered images, a common problem is how to handle the
background. If the model is rendered over a natural image background, gradients
in the background will be incorporated into the discriminative template,
potentially causing false-positives if background elements exist in the query
image.

Alternatively, if the background is left textureless (see
Fig.~\ref{fig:rendering}), whitening the resulting HOG template
will produce undesirable background artifacts. These are created when centering
the template (by subtracting the mean $\mu$), where strong negative weights are
introduced in the textureless region (as seen in Fig.~\ref{fig:whocomparison}).
This could result in positive matches being suppressed due to spurious
background gradients.

NZ-WHO removes these artifacts so that the background has no effect on the
template response. Given HOG features $x$, we create a new vector $x'$ which
contains only the non-zero elements of $x$. We select the same elements of
$\mu$ to produce $\mu'$, while the autocovariance lookup is only performed for
the cells with non-zero gradient, producing $\Sigma'$ corresponding to the
features in $x'$. After solving the resulting system (which is now smaller than
in the WHO approach) we find

\begin{equation}
w'=\Sigma'^{-1}(x' - \mu') \label{eq:nz-who},
\end{equation}which we reassemble into the final NZ-WHO template $w$ by
replacing the non-zero elements of $x$ with the corresponding $w'$ element.

% Notice that $\Sigma$ is not square-rooted in Eq.~\ref{eq:nz-who}, whereas it is
% in the definition of whitening (Eq.~\ref{eq:whitening}). This is because the
% whitened template will only be used in the context of detection, which involves
% the inner product with the whitened HOG features of the query image $y$:
% \begin{equation}
% (y-\mu)^{T}\Sigma^{-\frac{1}{2}}\Sigma^{-\frac{1}{2}}(x-\mu)\\
% = (y-\mu)^{T} w.
% \end{equation}
% We can therefore capture the whitening of the query image features in $w$.

% We use the GPU to parallelize the lookup of the correct autocovariance cells during covariance matrix syntheses, which is \scream{x} times faster than using the CPU (Figure \ref{fig:covariancetime}).

%\scream{chris:We propose Non-Zero Whitened Histograms of Orientations (NZ-WHO), an improvement over WHO which is both more discriminative than WHO and can be computed many orders of magnitude faster than WHO (70ms as opposed to to several seconds for WHO). This speedup allows us to generate NZ-WHO templates on the fly, allowing us to quickly evaluate arbitrary viewpoint hypotheses.}
\scream{add comments about the fig:whocomparison}

\begin{figure}[t]
  \begin{center}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    % include whitening all centered cells
    % \includegraphics[width=0.32\linewidth]{whiten_all_crop}
    \setlength\tabcolsep{3pt}
    \begin{tabular}{ccc}
      HOG & WHO & NZ-WHO \\
%     \begin{turn}{90}$w_+$\end{turn} &
    \includegraphics[width=0.28\linewidth]{hog_crop} &
    \includegraphics[width=0.28\linewidth]{whiten_all_crop} &
    \includegraphics[width=0.28\linewidth]{whiten_non_zero_crop} \\
    % include whitening all centered cells
     % \includegraphics[width=0.282\linewidth]{whiten_all_neg_crop} 
%     \begin{turn}{90}$-w_-$\end{turn} &
     \includegraphics[width=0.28\linewidth]{hog_neg_crop} &
     \includegraphics[width=0.28\linewidth]{whiten_all_neg_crop}  &
     \includegraphics[width=0.28\linewidth]{whiten_non_zero_neg} \\
    % include whitening all centered cells
 %    \begin{turn}{90}ihog\cite{vondrick2013}\end{turn} &
    % \cite{vondrick2013}& 
     \includegraphics[width=0.28\linewidth]{ihog_hog200_crop.png} &
     \includegraphics[width=0.28\linewidth]{ihog_whiten_all200_crop.png} &
     \includegraphics[width=0.28\linewidth]{ihog_whiten_non_zero200_crop.png} \\
 \end{tabular}
  \end{center}
  \caption{Comparison of HOG, WHO and NZ-WHO. Visualization of positive weights (first row),  visualization of negative weights (second row), HOGgles \cite{vondrick2013} (third row). Note that for WHO, whitening all cell result in strong negative edges on the empty region}
  \label{fig:whocomparison}
\end{figure}


\subsubsection{Fast Whitening using Conjugate Gradient}

The computational efficiency of our approach comes from using the iterative
Conjugate Gradient method to whiten a HOG template. Whitening the non-zero
feature vector $x'$ means a solving the system of linear equations, $\Sigma' w' = (x' -
\mu')$. In
\cite{Hariharan12}, the authors make use of the fact that covariance matrices
are symmetric and positive semidefinite to solve the system via the Cholesky
decomposition with Gaussian Elemination, which requires $O(n^3)$ time \scream{Citation for complexity}.

The Conjugate Gradient method is an iterative algorithm for solving symmetric
positive definite systems which runs in $O(n^2\kappa)$ time, where $\kappa$ is
the condition number of the matrix. \scream{Citation for complexity}.

This makes Conjugate Gradient faster than decomposition for matrices with small condition
numbers relative to their size.

The covariance matrix for HOG templates is typically ill-conditioned\cite{Hariharan12}, but we
can add a regularization constant to the diagonal to reduce its condition
number to the point where the conjugate gradient method will converge quickly.
We use a constant of $0.15$, which reduces the condition number from $10^{20}$
to $50$, much smaller than the dimension of the matrix (7000).

As a result, a GPU implementation of conjugate gradient converges in 50
ms when using 250 HOG cells, two orders of magnitude faster than the using
Cholesky factorization with Gaussian Elemination. We report the real time
analysis of whitening using decomposition and conjugate gradient methods in
Fig.~\ref{fig:whotime}

We compare the speed of each of template generation methods in
Fig.~\cite{fig:covariancetime_crop} Using naive Cholesky decomposition,
WHO template takes several seconds. But, if we use iterative Conjugate Gradient method, it only
takes 100ms. If we use NZ-WHO, we can gain extra speed up since we only whitens non-zero cells.


Also, since the iterative Conjugate Gradient method directly tries to reduce the residual (the norm of $y-Ax$ for $Ax = y$),
it is more numerically stable than Cholesky decomposition with Gaussian Elemination. We vary the number of cells in a template
and show that the residual of NZ-WHO is smaller than WHO Fig.~\ref{fig:whoresidual}.

% Also, the residual from the Conjugate Gradient method (the norm of $y-Ax$ for $Ax = y$) is smaller than 
% that of Cholesky decomposition. 

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{cc}
     \includegraphics[width=0.5\linewidth]{whotime} & 
     \includegraphics[width=0.5\linewidth]{speedup}\\
     (a) & (b) \\
 \end{tabular}
  \end{center}
  \caption{(a) Realtime analysis of whitening. HOG means feature extraction time, WHO-Chol uses method of \cite{Hariharan12} and WHO-CG uses iterative Conjugate Gradient, and NZ-WHO-CG uses only non-zero cells and use Conjugate Gradient. (b) final speed of using NZ-WHO compare to WHO}
  \label{fig:whotime}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{residual}
  \caption{Residuals from different method}
  \label{fig:whoresidual}
\end{figure}

\subsection{High Resolution Templates and FFT based Convolution}
\label{sec:fft} 
We generate high resolution templates with more than 250 HOG cells to capture
details of an object to give accurate 2D-3D matching. These large templates
cause computational burden when computing convolution. Though good for
detecting accurate model and pose, convolution of high-resolution templates are
much slower since computation time scales linearly with the number of HOG
cells in the template. To overcome and speed up the convolution, we used FFT
based GPU convolution \cite{Podlozhnyuk} which scales. Briefly, for length $n$ signal and
length $m$ filter, naive convolution takes $O(nm)$ time whereas FFT-based
convolution takes $O\left( (n + m)\log (n+m) \right)$ time. For large $m$, high
resolution template, we can gain computational advantage.

\subsection{Variations of WHO}

To validate our approach, we run an ensemble of NZ-WHO templates as a bank of exemplar detectors and compare its performance with other WHO variants. Since WHO variants uses HOG features, they are slower than HOG templates but WHO variants in general performs better than WHO by significant margin. However, WHO using Cholesky decomposition with Gaussian Elemination takes the most time yet performs worse than NZ-WHO which is two order of magnitude faster to generate. 

We also calibrated the various methods

In the experiment, we compared the performance of HOG, WHO, WHO-CG, WHO-CG-Z and NZ-WHO. The WHO refers to standard WHO using the method presented in \cite{Hariharan12}, WHO-CG uses iterative Conjugate Gradient method to generate WHO. WHO-CG-Z uses whiten the whole template and zero out textureless region. NZ-WHO-CG is the NZ-WHO which whitens only non-zero cells using iterative Conjugate Gradient method. The time column indicates the time to generate one template. We followed calibration procedure presented in \cite{Aubry14}

The experiment results is on Tab.~\ref{tab:who_initializations}
We found out that without calibration, NZ-WHO performs the best on object
detection on 3DObject car dataset \cite{Savarese07}. See Table
\ref{tab:who_initializations} for detail. In essence, if we use calibration, we
could achieve better performance but since our goal is to do 2D-3D matching and
continuous viewpoint estimation using on-the-fly template generation, we did not
calibrated our templates.


We empirically found out that NZ-WHO performs reasonably well without time
consuming calibration stage compare to other variational method to generate
templates. We used 1 CAD model with viewpoints covering 24 azimuths and 4
elevations. Each template takes approximately 80 milliseconds to generate a
NZ-WHO template. We also calibrated templates using the method presented in
\cite{Aubry14}. The calibration learns affine transformation of the detection
confidence.% Note that in Section \ref{experiments}, we provide the same
% experiment with 9 CAD models.


\begin{table*}[!htbp]
    \footnotesize
    \begin{center}
\begin{tabular}{|c|c|r|c|r|}
\hline
Methods (AP/MPPE) & before calibration  & time & after calibration \cite{Aubry14} & time \\
\hline\hline
HOG\cite{Dalal05}     & 72.3 / 65.0           &  31ms  & 60.4 / 50.2                 & 8.7 sec \\ 
WHO\cite{Hariharan12} & 82.1 / 85.4           &  3811ms& 84.4 / 83.0                 & 12.4 sec  \\
WHO-CG                & 81.7 / \textbf{84.9}  &  104ms & 83.7 / 87.3                 & 8.3 sec \\
WHO-CG-Z              & 54.4 / 65.1           &  103ms & \textbf{92.8} / 86.7        & 8.7 sec  \\
% NZC-WHO-Z    & 89.10/\textbf{78.64} &    & 91.15/74.79                  &     \\
NZ-WHO-CG             & \textbf{90.0} / 82.8  &   79ms & 90.3 / \textbf{86.8}        & 8.5 sec   \\
\hline
\end{tabular}
\end{center}
\caption{Average Precision(AP) and Mean Precision in Pose Estimation (MPPE) \cite{Lopez-Sastre11} variations of WHO on 3DObject Car dataset\cite{Savarese07}. WHO refers to standard WHO using the method presented in \cite{Hariharan12}, WHO-CG uses iterative Conjugate Gradient method to generate WHO. WHO-CG-Z uses whiten the whole template and zero out textureless region. NZ-WHO-CG is the NZ-WHO which whitens only non-zero cells using iterative Conjugate Gradient method. The time column indicates the time to generate one template. We followed calibration procedure presented in \cite{Aubry14}.}
\label{tab:who_initializations}
\end{table*}


\section{Estimating Parameters using MCMC}
\label{sec:fine}
% Estimating viewpoint continuously to fine tune a initial matching
The NZ-WHO template matching method we have presented makes template generation
and evaluation computationally inexpensive. This means we can efficiently
explore the parameter space to find the best object pose, scale, model type and
camera focal length.

Since creating a template and validating the template become computationally
inexpensive, making a proposal and validating the proposal to fine tune a 2D-3D
matching became affordable. We used Metropolis-Hastings algorithm to generate a
proposal pose and focal length and validate the proposal using the convolution
and returning maximum response as a return.

To fine-tune continuous pose and location of object, we require strong
initialization. The initialization is a rough bounding box with small context
and viewpoint. Since most of the standard detectors does not provide viewpoint
of an object, we first run an ensemble of NZ-WHO templates to get rough
viewpoints. Once we get rough viewpoint, we initialize the proposal $x_0$ and
start sampling proposals.

We modeled the distribution of object location and pose as an exponential distribution. 
For proposal distribution, we used Gaussian distribution to propose next viewpoint and for viewpoint proposals
and uniform distribution for model change proposals.  

More formally, suppose that $x = [p, m, f]$ where $p$ is the 3D rotation of the CAD model and
$m$ is the CAD model index and $f$ is the focal length. We use Metropolis Hastings
to optimize $x$.

\begin{align}
    A(x \rightarrow x') & =  min\left( 1,  \frac{P(x' g(x') \rightarrow x)}{P(x) g(x \rightarrow x')}\right) \\
    g(x \rightarrow x + p_i') & \sim \mathcal{N}(\mu = 0,\sigma = c_1) \quad i \in \{1,2,3\}\\
    g(x \rightarrow x + f') & \sim \mathcal{N}(\mu = 0,\sigma = c_2)\\
    g(x \rightarrow x + m') & \sim c_3 \delta(m) + (1-c_3) Unif(1,M)
\end{align}


Where $g$ is the proposal distribution and $P$ is the distribution of which we
defined using Gaussian distribution around previous state. $A$ is the
acceptance probability where we accept new proposal with $A(x \rightarrow x')$.

\begin{align}
    P(x) & \sim e^{ c_4 \max_{s} w(x) \ast \mathcal{T}_s(\mathcal{I}) }
\end{align}

$P$ is the distribution of the pose and location of an object on image
$\mathcal{I}$. One can also think of the image as an image cropped by detection
bounding box. We synthesize covariance matrix and create NZ-WHO template $w(x)$ for particular viewpoint, focal length and CAD model.

We initialize the $x$ using the result from the previous stage where we run our ensemble or NZ-WHO templates to get rough estimate.

In sum, for new proposal $x'$, we make a NZ-WHO template on the fly 
We found out that it requires good initialization to converge to local minima. 

We represented the probability using exponential family. 

So our model favors large convolution score $\max_{s} w(x) \ast T(\mathcal{I})$. 
% there are several advantages that was not possible such as \textbf{on-the-fly} template generation and making large number of templates  
\begin{figure}[t]
\centering
    \includegraphics[width=0.7\linewidth]{tuning2} \\ [-5pt]
    \caption{Different modes of Metropolis Hasting proposals, see Section \ref{sec:fine}}
 \label{fig:tuningmode}
\end{figure}
    



\section{Experiments}
\label{sec:experiments}
\input{experiments}



\section{Conclusion}


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
