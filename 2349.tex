\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{algorithm2e}
\usepackage{rotating}
\usepackage[space]{grffile}
\usepackage[font=small,skip=0pt]{caption}
%\DeclareMathOperator{\Tr}{Tr}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\graphicspath{ {figures/} }
\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2349} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% custom commands
\newcommand{\scream}[1]{{\color{red} \bf *** #1 ***}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Enriching Object Detection with 2D-3D Registration\\and Continuous Viewpoint Estimation}
%\title{Enrich Object Detection : 2D-3D registration and continuous viewpoint estimation}

\author{Christopher Bongsoo Choy\textsuperscript{\dag}, Michael Stark\textsuperscript{\dag \ddag}, Sam Corbett-Davies\textsuperscript{\dag}, Silvio Savarese\textsuperscript{\dag}\\
	\textsuperscript{\dag}Stanford University, \textsuperscript{\ddag}Max Planck Institute for Informatics\\
	{\tt\small \{chrischoy, scorbett, ssilvio\}@stanford.edu, \textsuperscript{\ddag}stark@mpi-inf.mpg.de }
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
A large body of recent work on object detection has focused on exploiting 3D CAD
model databases to improve detection performance. Many of these approaches work
by aligning exact 3D models to images using templates generated from renderings
of the 3D models at a set of discrete viewpoints. However, the training
procedures for these approaches are computationally expensive and require
gigabytes of memory and storage, while the viewpoint discretization hampers
pose estimation performance.

We propose an efficient method for synthesizing templates from 3D models that
runs on the fly -- that is, it quickly produces detectors for an arbitrary
viewpoint of a 3D model without expensive dataset-dependent training or template
storage. Given a 3D model and an arbitrary continuous detection viewpoint, our
method synthesizes a discriminative template by extracting features from a
rendered view of the object and decorrelating spatial dependences among the
features. Our decorrelation procedure relies on a gradient-based algorithm that
is more numerically stable than standard decomposition-based procedures, and we
efficiently search for candidate detections by computing FFT-based template
convolutions. Due to the speed of our template synthesis procedure, we are able
to perform joint optimization of scale, translation, continuous rotation, and
focal length using Metropolis-Hastings algorithm. We provide an efficient GPU
implementation of our algorithm, and we validate its performance on 3D Object
Classes and PASCAL3D+ datasets.

%     Using 3D model to detect and register the model to RGB image has been a
%     growing field of study. Recently, Aubry \etal \cite{Aubry14} and J. Lim
%     \etal \cite{Lim14} tried to estimate pose and align exact CAD model to an
%     image using part-based templates. Malisiewicz \etal \cite{Malisiewicz11}
%     also addresses the issue by transferring metadata after detection. Hejrati
%     \etal \cite{Hejrati14} uses template synthesis to detect an object. All
%     these approaches require extensive training, a lot of memory space. Also,
%     the training based approach requires defining fixed viewpoints which limit
%     themselves.  To overcome these difficulties, we propose an efficient and
%     fast way to synthesize and validate templates using on the fly realistic
%     rendering that can jointly optimize scale, translation, rotation and focal
%     length continuously. We render an object and extract features and
%     decorrelate spatial dependencies within them on the fly to make a
%     discriminative template. To speed up the decorrelation procedure, we adopt
%     a gradient based algorithm which is numerically more stable than
%     decomposition procedure. Finally, we efficiently search for candidate
%     match by computing convolution using FFT and jointly optimize to match the
%     object accurately. To speed up further, we propose an efficient GPU
%     implementation and tested on PASCAL images.

%   In contrast to other mid-level patch based 3D matching methods which
%   requires extensive training and more than 10Gb of memory space and physical
%   storage, ours require no training nor saving the templates thus enabling it
%   to run on any personal computer.  To overcome these difficulties, we propose
%   an efficient and fast way to synthesize and validate templates using on the
%   fly realistic rendering that can jointly optimize scale, translation,
%   rotation and focal length continuously. We render an object and extract
%   features and decorrelate spatial dependencies within them on the fly to make
%   a discriminative template. To speed up the decorrelation procedure, we adopt
%   a gradient based algorithm which is numerically more stable than
%   decomposition procedure. Finally, we efficiently search for candidate match
%   by computing convolution using FFT and jointly optimize to match the object
%   accurately. To speed up further, we propose an efficient GPU implementation
%   and tested on PASCAL images.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{intro}
%
\section{Related Work}
\label{sec:related}
\input{related}

\section{Approach Overview}
\label{sec:nz-who}
\input{overview}

\section{On-the-Fly Template Generation}
\label{sec:approach}
\input{approach}

\section{Pose Fine-Tuning via MCMC}
\label{sec:fine}
\input{mcmc}

\section{Experiments}
\label{sec:experiments}
\input{experiments}

\section{Conclusion}
\label{conclusion}
\vspace{-.2cm}
\input{conclusion}


\paragraph{Acknowledgement}
This work has been supported by the Max Planck Center for Visual Computing \& Communication.

\clearpage
\clearpage
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
