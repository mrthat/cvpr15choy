% Use class option [extendedabs] to prepare the 1-page extended abstract.
\documentclass[extendedabs]{bmvc2k}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\graphicspath{ {figures/} }

% Document starts here
\begin{document}


\title{Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint Estimation}
\addauthor{
Christopher Bongsoo Choy\textsuperscript{\dag}, Michael Stark\textsuperscript{\dag \ddag}, Sam Corbett-Davies\textsuperscript{\dag}, Silvio Savarese\textsuperscript{\dag}}{}{1}
\addinstitution{
\textsuperscript{\dag}Stanford University, \textsuperscript{\ddag}Max Planck Institute for Informatics
}


\maketitle
\let\thefootnote\relax\footnote{This is an extended abstract. The full paper is available at the \href{http://www.cv-foundation.org/openaccess/CVPR2015.py}{Computer Vision Foundation webpage}. }
\vspace{-0.2in}


% Extended abstract begins here.  In a one-page document, there is
% little need for section headers, but you may use \section etc if you
% wish.

\noindent
% In this paper, we propose a novel method for 2D-3D alignment of exemplar CAD
% models to real-world images that circumvents the need for calibration and
% greatly enhances the scalability of WHO. Our model generates a discriminative
% exemplar template on-the-fly and align model accurately using MCMC.

A large body of recent work on object detection has focused on providing more
than a bounding box of an object. Some exploited 3D CAD
model databases to provide viewpoint estimation. These approaches work
by aligning exact 3D models to images using templates generated from renderings
of the 3D models at a set of discrete viewpoints.

However, training the template for all viewpoint and CAD models is not feasible
since gathering all training data for different viewpoint and CAD models is
expensive.  Instead, it has been realized that template-based exemplar
detectors based on HOG features can be trained analytically, by
replacing the standard SVM with an LDA classifier $w_{x_t}$ for a template $x_t$
~\cite{Hariharan12}.

The result is a whitened feature representation, termed WHO (Whitened Histogram
of Orientations). This development makes it feasible to train hundreds of
thousands of mid-level patch detectors for recognition.

Though it achieved remarkable improvements, generating LDA templates (matrix
decomposition with Gaussian Elimination) and calibrating them are
computationally expensive and require gigabytes of memory and storage, while
the viewpoint discretization hampers pose estimation performance.

% In this paper, instead of caching and calibrating all templates on training
% time, we selectively generate templates on testing time as well to estimate
% viewpoint continuously using the technique we proposed termed Non-Zero Whitened
% Histograms of Orientations (NZ-WHO).

% The method uses Conjugate Gradient method to efficiently generate an LDA template on-the-fly (60 ms) without expensive calibration steps.

\begin{figure}[t]
  \centering
  % \includegraphics[width=0.9\linewidth]{schematics} 
  \includegraphics[width=0.8\linewidth]{front} % \\[-5pt]
  \caption{Using a database of 3D CAD models, we generate NZ-WHO
    templates which can be used to either detect objects directly or
    enrich the output of an existing detector with high-quality,
    continuous pose and 3D CAD model exemplar.}
    % initialize pose of an object in the bounding box from other
    % detectors. Once initialization is given, we use NZ-WHO to propose
    % and validate plausible pose on-the-fly and
    % further tune the translation, 3D viewpoint and focal length continuously.}
  % \vspace{-1em}
  \label{fig:front}
\end{figure}


\paragraph{Overview}

In this paper, we propose a novel method for 2D-3D alignment of exemplar CAD
models to real-world images that circumvents the need for calibration and
greatly enhances the scalability of WHO. As a result, we can render novel views
and train corresponding exemplar models {\em on-the-fly}, without the need for
offline processing. We call these Non-Zero Whitened Histograms of Orientations
(NZ-WHO) templates Fig.~\ref{fig:front}. As a by-product, we can formulate the
alignment problem as a parameter search in a continuous pose space, consisting
of yaw, pitch and roll, which we implement using MCMC sampling.

First, we present a novel method for training exemplar models from
rendered 3D CAD data on-the-fly, enabled by a novel variant of
WHO, termed Non-Zero Whitened Histograms of Orientations (NZ-WHO), and making
efficient use of the specific characteristics of rendered images. To our
knowledge, our method constitutes the first attempt to simultaneously render
and train exemplar detectors on-the-fly.

Second, we demonstrate that our method can enrich the output of an existing
object class detector, such as the DPM or the
R-CNN with additional 3D information. By applying our
method to candidate detections provided by the respective detector, we can
augment the original detections with an estimate of 3D continuous pose and a 3D
CAD model exemplar.

Finally, we give an in-depth experimental study that demonstrates
the effectiveness of our approach on a standard benchmark for
object detection and viewpoint estimation~\cite{Xiang14}.


\begin{figure}[t]
\small
\setlength\tabcolsep{1pt}
\centering
\begin{tabular}{ccccc}
  \includegraphics[width=0.2\textwidth]{car_cnn/2c.png} &
  \includegraphics[width=0.2\textwidth]{car_cnn/2e.png} \\
  \includegraphics[width=0.2\textwidth]{bicycle_cnn/4b.png} &
  \includegraphics[width=0.2\textwidth]{bicycle_cnn/4c.png} \\
\end{tabular}
\caption{Example enriched bounding boxes. Given R-CNN
	detection bounding boxes, our method predicts the best 2D-3D
	registration given the bounding box input. Blue boxes are R-CNN output and
	purple boxes are the tightest bounding box enclosing predicted CAD model.}
\label{fig:pascal12cnn}
\end{figure}


\paragraph{Non-Zero Whitened Histograms of Orientations}

NZ-WHO removes these artifacts so that the background has no effect on the
template response. Let a vectorized HOG feature of a rendering image $x = [
x_1^T ,  \cdots , x_n^T ]^T \in \mathcal{R}^{nd}$ where $x_i$ is the $i$th HOG cell feature, $n$ is the
number of HOG cells and $d$ is the dimension of the HOG feature. We create a new
vector $\bar{x}$ which contains only the non-zero HOG cell features of $x$.
Synthesizing the LDA template requires solving the system of linear equations
$\bar{w}=\bar{\Sigma}^{-1}(\bar{x} - \bar{\mu})$. In \cite{Hariharan12}, the
authors solve the system via the Cholesky decomposition, which requires
$O(n^3)$ time. Instead, we introduced the Conjugate Gradient method to generate
LDA templates. The method runs in $O(n^2\kappa)$ time, where $\kappa$ is the condition number of the matrix and achieved more than 100x speed-up.

% Also, the residual from the Conjugate Gradient method (the norm of $y-Ax$ for
% $Ax = y$) is smaller than 
% that of Cholesky decomposition. 

\paragraph{Fine-Tuning using MCMC}
Due to the speed of our template synthesis procedure, we are able to perform
joint optimization of scale, translation, continuous rotation, and focal length
using the Metropolis-Hastings algorithm.

We model the probability of an object with the parameter $\theta$ in the test
image $\mathcal{I}$ as $P(\theta| \mathcal{I}) \sim e^{ \max_{s} w(\theta)
	\ast \mathcal{T}_s(\mathcal{I})}$. We approximate the MAP solution for $p$ by
drawing samples from the distribution $P(\theta | \mathcal{I})$, using the
Metropolis-Hastings algorithm.

\paragraph{Experiments}

First, we verify that our NZ-WHO method delivers performance that is
at least on par with the original WHO formulation~\cite{Hariharan12} in terms of
accuracy, while at the same time resulting in large computational
savings.

Second, we demonstrate that our method can be used for multi-view
object class detection in isolation. It can be applied in a sliding
window fashion and deliver 2D bounding box as well as viewpoint
information. Our method is competitive with the state-of-the-art in this
case.

Finally, we show that our method can be used to complement the
detections provided by an existing object class detector, such
as DPM or RCNN. In this case, we show a considerable performance improvement
compared to previous work in the task of joint object class detection and VP
estimation. Example outputs of our model are on Fig.~\ref{fig:pascal12cnn}.

\bibliography{egbib}

\end{document}
